{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autotvm_conv2d_cuda.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/eqy/8ea2aff4c7b8d6d95e9e2c0db6865f1d/autotvm_conv2d_cuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8T0-Z4MasbY",
        "colab_type": "text"
      },
      "source": [
        "Tuning High Performance Convolution on NVIDIA GPUs\n",
        "=========================================================================\n",
        "**Author**: `Lianmin Zheng <https://github.com/merrymercy>`_\n",
        "\n",
        "Adapted by `Eddie Yan <https://github.com/eqy>`_\n",
        "\n",
        "This is an advanced tutorial for writing high performance tunable template for\n",
        "NVIDIA GPU. By running auto-tuner on this template, we can outperform the\n",
        "vendor provided library CuDNN in many cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9soFF1q_cV-y",
        "colab_type": "text"
      },
      "source": [
        "Please run the following block to ensure TVM is setup for *this notebook*, each notebook may have its own runtime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_lzOGuFcUgG",
        "colab_type": "code",
        "outputId": "a2ba6091-4e48-44a7-d855-dc03b651fbb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2757
        }
      },
      "source": [
        "! gsutil cp \"gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz\" /tmp/tvm.tar.gz\n",
        "! mkdir -p /tvm\n",
        "! tar -xf /tmp/tvm.tar.gz --strip-components=4 --directory /tvm\n",
        "! ls -la /tvm\n",
        "# Move this block after we are done with pkg step\n",
        "! bash /tvm/package.sh\n",
        "import sys\n",
        "sys.path.append('/tvm/python')\n",
        "sys.path.append('/tvm/topi/python')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz...\n",
            "/ [0 files][    0.0 B/112.9 MiB]                                                \r-\r- [1 files][112.9 MiB/112.9 MiB]                                                \r\n",
            "Operation completed over 1 objects/112.9 MiB.                                    \n",
            "total 164\n",
            "drwxr-xr-x 21 root root  4096 Jun 11 00:55 .\n",
            "drwxr-xr-x  1 root root  4096 Jun 11 00:55 ..\n",
            "drwx------  8 root root  4096 May 31 08:14 3rdparty\n",
            "drwx------ 12 root root  4096 May 31 08:14 apps\n",
            "drwx------  3 root root  4096 Jun  4 09:46 build\n",
            "drwx------  4 root root  4096 May 31 08:14 cmake\n",
            "-rw-------  1 root root 10406 May 31 08:14 CMakeLists.txt\n",
            "drwx------  6 root root  4096 May 31 08:14 conda\n",
            "-rw-------  1 root root  5673 May 31 08:14 CONTRIBUTORS.md\n",
            "drwx------  3 root root  4096 May 31 08:14 docker\n",
            "drwx------ 11 root root  4096 May 31 08:14 docs\n",
            "drwx------  4 root root  4096 May 31 08:14 golang\n",
            "drwx------  3 root root  4096 May 31 08:14 include\n",
            "-rw-------  1 root root 10027 May 31 08:14 Jenkinsfile\n",
            "drwx------  6 root root  4096 May 31 08:14 jvm\n",
            "-rw-------  1 root root 11357 May 31 08:14 LICENSE\n",
            "-rw-------  1 root root  4267 May 31 08:14 Makefile\n",
            "-rw-------  1 root root 10476 May 31 08:14 NEWS.md\n",
            "drwx------  9 root root  4096 May 31 08:14 nnvm\n",
            "-rw-------  1 root root    61 May 31 08:14 NOTICE\n",
            "-rw-------  1 root root   369 Jun  3 22:49 package.sh\n",
            "drwx------  3 root root  4096 May 31 08:14 python\n",
            "-rw-------  1 root root  2705 May 31 08:14 README.md\n",
            "drwx------  5 root root  4096 May 31 08:14 rust\n",
            "drwx------ 14 root root  4096 May 31 08:14 src\n",
            "drwx------  9 root root  4096 May 31 08:14 tests\n",
            "drwx------  7 root root  4096 May 31 08:14 topi\n",
            "drwx------  8 root root  4096 May 31 08:14 tutorials\n",
            "-rw-------  1 root root  2902 May 31 08:14 version.py\n",
            "drwx------ 10 root root  4096 May 31 08:14 vta\n",
            "drwx------  2 root root  4096 May 31 08:14 web\n",
            "Installing Dependencies ...\n",
            "deb https://dl.bintray.com/sbt/debian /\n",
            "Executing: /tmp/apt-key-gpghome.O5wEDU9L8s/gpg.1.sh --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823\n",
            "gpg: key 99E82A75642AC823: public key \"sbt build tool <scalasbt@gmail.com>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [490 kB]\n",
            "Get:11 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Ign:12 https://dl.bintray.com/sbt/debian  InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [834 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [324 kB]\n",
            "Get:15 https://dl.bintray.com/sbt/debian  Release [815 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,217 kB]\n",
            "Get:17 https://dl.bintray.com/sbt/debian  Release.gpg [821 B]\n",
            "Get:18 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [3,902 B]\n",
            "Get:22 https://dl.bintray.com/sbt/debian  Packages [3,424 B]\n",
            "Get:23 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,647 kB]\n",
            "Get:24 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [56.4 kB]\n",
            "Get:25 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [790 kB]\n",
            "Fetched 5,638 kB in 3s (1,968 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "clinfo is already the newest version (2.2.18.03.26-1).\n",
            "libtinfo-dev is already the newest version (6.1-1ubuntu1.18.04).\n",
            "libtinfo-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  llvm-6.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  binfmt-support libffi-dev llvm-6.0 llvm-6.0-dev llvm-6.0-runtime\n",
            "0 upgraded, 5 newly installed, 0 to remove and 80 not upgraded.\n",
            "Need to get 28.2 MB of archives.\n",
            "After this operation, 178 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 binfmt-support amd64 2.1.8-2 [51.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0-runtime amd64 1:6.0-1ubuntu2 [200 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0 amd64 1:6.0-1ubuntu2 [4,838 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libffi-dev amd64 3.2.1-8 [156 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0-dev amd64 1:6.0-1ubuntu2 [23.0 MB]\n",
            "Fetched 28.2 MB in 1s (47.8 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package binfmt-support.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack .../binfmt-support_2.1.8-2_amd64.deb ...\n",
            "Unpacking binfmt-support (2.1.8-2) ...\n",
            "Selecting previously unselected package llvm-6.0-runtime.\n",
            "Preparing to unpack .../llvm-6.0-runtime_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0-runtime (1:6.0-1ubuntu2) ...\n",
            "Selecting previously unselected package llvm-6.0.\n",
            "Preparing to unpack .../llvm-6.0_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0 (1:6.0-1ubuntu2) ...\n",
            "Selecting previously unselected package libffi-dev:amd64.\n",
            "Preparing to unpack .../libffi-dev_3.2.1-8_amd64.deb ...\n",
            "Unpacking libffi-dev:amd64 (3.2.1-8) ...\n",
            "Selecting previously unselected package llvm-6.0-dev.\n",
            "Preparing to unpack .../llvm-6.0-dev_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0-dev (1:6.0-1ubuntu2) ...\n",
            "Setting up binfmt-support (2.1.8-2) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/binfmt-support.service â†’ /lib/systemd/system/binfmt-support.service.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libffi-dev:amd64 (3.2.1-8) ...\n",
            "Setting up llvm-6.0-runtime (1:6.0-1ubuntu2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up llvm-6.0 (1:6.0-1ubuntu2) ...\n",
            "Processing triggers for systemd (237-3ubuntu10.21) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up llvm-6.0-dev (1:6.0-1ubuntu2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  gtkwave systemc\n",
            "The following NEW packages will be installed:\n",
            "  sbt verilator\n",
            "0 upgraded, 2 newly installed, 0 to remove and 80 not upgraded.\n",
            "Need to get 4,005 kB of archives.\n",
            "After this operation, 14.4 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 verilator amd64 3.916-1build1 [2,878 kB]\n",
            "Get:2 https://dl.bintray.com/sbt/debian  sbt 1.2.8 [1,126 kB]\n",
            "Fetched 4,005 kB in 1s (4,815 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package sbt.\n",
            "(Reading database ... 132546 files and directories currently installed.)\n",
            "Preparing to unpack .../apt/archives/sbt_1.2.8_all.deb ...\n",
            "Unpacking sbt (1.2.8) ...\n",
            "Selecting previously unselected package verilator.\n",
            "Preparing to unpack .../verilator_3.916-1build1_amd64.deb ...\n",
            "Unpacking verilator (3.916-1build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up verilator (3.916-1build1) ...\n",
            "Setting up sbt (1.2.8) ...\n",
            "Creating system group: sbt\n",
            "Creating system user: sbt in sbt with sbt daemon-user and shell /bin/false\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agr421w8a49k",
        "colab_type": "text"
      },
      "source": [
        "Import packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9JnukA4aTLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "import tvm\n",
        "import topi\n",
        "from topi.testing import conv2d_nchw_python\n",
        "\n",
        "from tvm import autotvm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5YlgPYma_5r",
        "colab_type": "text"
      },
      "source": [
        "Step 0: Vanilla direct 2D convolution implementation without a tunable template\n",
        "---------------------------------------------------------------------------------------------\n",
        "\n",
        "We reuse the conv2d with NCHW data layout in the TVM operator inventory (TOPI).\n",
        "This definition gives us the default schedule (loop nest) seen below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhtW-j3bbMLI",
        "colab_type": "code",
        "outputId": "dd543139-b55e-43aa-9f7d-975d9af84bab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "# the last layer in resnet\n",
        "N, H, W, CO, CI, KH, KW, stride, padding = 1, 7, 7, 512, 512, 3, 3, (1, 1), (1, 1)\n",
        "assert N == 1, \"Only consider batch_size = 1 in this template\"\n",
        "\n",
        "data = tvm.placeholder((N, CI, H, W), name='data')\n",
        "kernel = tvm.placeholder((CO, CI, KH, KW), name='kernel')\n",
        "conv = topi.nn.conv2d_nchw(data, kernel, stride, padding, dilation=1, out_dtype='float32')\n",
        "s = tvm.create_schedule([conv.op])\n",
        "print(\"Default Schedule:\")\n",
        "print(tvm.lower(s, [data, kernel, conv], simple_mode=True))\n",
        "\n",
        "# assign axes of the default schedule to variables\n",
        "n, f, y, x = s[conv].op.axis\n",
        "rc, ry, rx = s[conv].op.reduce_axis"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default Schedule:\n",
            "// attr [pad_temp] storage_scope = \"global\"\n",
            "allocate pad_temp[float32 * 41472]\n",
            "produce pad_temp {\n",
            "  for (i1, 0, 512) {\n",
            "    for (i2, 0, 9) {\n",
            "      for (i3, 0, 9) {\n",
            "        pad_temp[((((i1*9) + i2)*9) + i3)] = tvm_if_then_else(((((1 <= i2) && (i2 < 8)) && (1 <= i3)) && (i3 < 8)), data[(((((i1*7) + i2)*7) + i3) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  for (ff, 0, 512) {\n",
            "    for (yy, 0, 7) {\n",
            "      for (xx, 0, 7) {\n",
            "        compute[((((ff*7) + yy)*7) + xx)] = 0.000000f\n",
            "        for (rc, 0, 512) {\n",
            "          for (ry, 0, 3) {\n",
            "            for (rx, 0, 3) {\n",
            "              compute[((((ff*7) + yy)*7) + xx)] = (compute[((((ff*7) + yy)*7) + xx)] + (pad_temp[((((((rc*9) + yy) + ry)*9) + xx) + rx)]*kernel[((((((ff*512) + rc)*3) + ry)*3) + rx)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYacZkkNd_xG",
        "colab_type": "text"
      },
      "source": [
        "Here, we inline padding into the computation (as opposed to padding in the input in a second operator) and declare cache stages. Cache stages are prepare a subset of the input (read) or output (write) for improved temporal locality with higher performance memories (e.g., registers and shared memory vs. global memory)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afD69lgUd-QG",
        "colab_type": "code",
        "outputId": "2670f209-14fd-4cd4-d443-3f7647541bd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1433
        }
      },
      "source": [
        "# inline padding\n",
        "pad_data = s[conv].op.input_tensors[0]\n",
        "s[pad_data].compute_inline()\n",
        "input = data\n",
        "data, raw_data = pad_data, data\n",
        "\n",
        "output = conv\n",
        "OL = s.cache_write(conv, 'local')\n",
        "\n",
        "# create cache stage\n",
        "AA = s.cache_read(data, 'shared', [OL])\n",
        "WW = s.cache_read(kernel, 'shared', [OL])\n",
        "AL = s.cache_read(AA, 'local', [OL])\n",
        "WL = s.cache_read(WW, 'local', [OL])\n",
        "\n",
        "print(tvm.lower(s, [input, kernel, conv], simple_mode=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "// attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "allocate pad_temp.shared[float32 * 41472]\n",
            "// attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "allocate pad_temp.shared.local[float32 * 41472]\n",
            "// attr [kernel.shared] storage_scope = \"shared\"\n",
            "allocate kernel.shared[float32 * 2359296]\n",
            "// attr [kernel.shared.local] storage_scope = \"local\"\n",
            "allocate kernel.shared.local[float32 * 2359296]\n",
            "// attr [compute.local] storage_scope = \"local\"\n",
            "allocate compute.local[float32 * 25088]\n",
            "produce pad_temp.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)] = tvm_if_then_else(((((1 <= ax2) && (ax2 < 8)) && (1 <= ax3)) && (ax3 < 8)), data[(((((ax1*7) + ax2)*7) + ax3) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce pad_temp.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared.local[((((ax1*9) + ax2)*9) + ax3)] = pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared.local {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared.local[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute.local {\n",
            "  for (ff.c, 0, 512) {\n",
            "    for (yy.c, 0, 7) {\n",
            "      for (xx.c, 0, 7) {\n",
            "        compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = 0.000000f\n",
            "        for (rc, 0, 512) {\n",
            "          for (ry, 0, 3) {\n",
            "            for (rx, 0, 3) {\n",
            "              compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = (compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] + (pad_temp.shared.local[((((((rc*9) + yy.c) + ry)*9) + xx.c) + rx)]*kernel.shared.local[((((((ff.c*512) + rc)*3) + ry)*3) + rx)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  for (ff, 0, 512) {\n",
            "    for (yy, 0, 7) {\n",
            "      for (xx, 0, 7) {\n",
            "        compute[((((ff*7) + yy)*7) + xx)] = compute.local[((((ff*7) + yy)*7) + xx)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDGX2K6MeWH6",
        "colab_type": "text"
      },
      "source": [
        "Here, we first grab the spatial axes from the schedule. Next, we define several magic numbers that are tiling factors that we use to split the original loop nest into one with several additional levels. We reorder the levels to redefine the computation order (and the memory access order) or the computation. As we will see in the next cell, this transformation also readies the schedule for a mapping from loop nests to GPU computation indicies (grids, blocks, threads)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLaKLAH7eWRH",
        "colab_type": "code",
        "outputId": "98b966ba-c1e7-4adf-e3c0-5fdb81867c51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1470
        }
      },
      "source": [
        "# tile spatial axes\n",
        "n, f, y, x = s[output].op.axis\n",
        "tile_f_factors = [8, 8, 8, 1]\n",
        "tile_x_factors = [7, 7, 7, 1]\n",
        "tile_y_factors = [7, 7, 7, 1]\n",
        "\n",
        "bf, vf = s[output].split(f, factor=tile_f_factors[1])\n",
        "vf, tf = s[output].split(vf, factor=tile_f_factors[2])\n",
        "tf, fi = s[output].split(tf, factor=tile_f_factors[3])\n",
        "\n",
        "by, vy = s[output].split(y, factor=tile_y_factors[1])\n",
        "vy, ty = s[output].split(vy, factor=tile_y_factors[2])\n",
        "ty, yi = s[output].split(ty, factor=tile_y_factors[3])\n",
        "\n",
        "bx, vx = s[output].split(x, factor=tile_x_factors[1])\n",
        "vx, tx = s[output].split(vx, factor=tile_x_factors[2])\n",
        "tx, xi, = s[output].split(tx, factor=tile_x_factors[3])\n",
        "\n",
        "kernel_scope = n  # this is the scope to attach global config inside this kernel\n",
        "\n",
        "s[output].reorder(n, bf, by, bx, vf, vy, vx, tf, ty, tx, fi, yi, xi)\n",
        "print(tvm.lower(s, [input, kernel, conv], simple_mode=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "// attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "allocate pad_temp.shared[float32 * 41472]\n",
            "// attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "allocate pad_temp.shared.local[float32 * 41472]\n",
            "// attr [kernel.shared] storage_scope = \"shared\"\n",
            "allocate kernel.shared[float32 * 2359296]\n",
            "// attr [kernel.shared.local] storage_scope = \"local\"\n",
            "allocate kernel.shared.local[float32 * 2359296]\n",
            "// attr [compute.local] storage_scope = \"local\"\n",
            "allocate compute.local[float32 * 25088]\n",
            "produce pad_temp.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)] = tvm_if_then_else(((((1 <= ax2) && (ax2 < 8)) && (1 <= ax3)) && (ax3 < 8)), data[(((((ax1*7) + ax2)*7) + ax3) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce pad_temp.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared.local[((((ax1*9) + ax2)*9) + ax3)] = pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared.local {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared.local[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute.local {\n",
            "  for (ff.c, 0, 512) {\n",
            "    for (yy.c, 0, 7) {\n",
            "      for (xx.c, 0, 7) {\n",
            "        compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = 0.000000f\n",
            "        for (rc, 0, 512) {\n",
            "          for (ry, 0, 3) {\n",
            "            for (rx, 0, 3) {\n",
            "              compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = (compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] + (pad_temp.shared.local[((((((rc*9) + yy.c) + ry)*9) + xx.c) + rx)]*kernel.shared.local[((((((ff.c*512) + rc)*3) + ry)*3) + rx)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  for (ff.outer, 0, 64) {\n",
            "    for (ff.inner.inner.outer, 0, 8) {\n",
            "      for (yy.inner.inner.outer, 0, 7) {\n",
            "        for (xx.inner.inner.outer, 0, 7) {\n",
            "          compute[((((((ff.outer*8) + ff.inner.inner.outer)*7) + yy.inner.inner.outer)*7) + xx.inner.inner.outer)] = compute.local[((((((ff.outer*8) + ff.inner.inner.outer)*7) + yy.inner.inner.outer)*7) + xx.inner.inner.outer)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH6gor8XjvXM",
        "colab_type": "text"
      },
      "source": [
        "After reshaping the loop nest, we can bind portions of the computations to GPU blocks and threads. Additionally, we bind some loops to \"virtual\" threads which are effectively threads emulated in software. Virtual threads enable the expression of more sophisticated computation and memory access patterns vs. blocks and threads alone. Note binding effictively removes the associated loop axes from the schedule, as they are now parallelized based on their index instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yIhMYiQjvhQ",
        "colab_type": "code",
        "outputId": "114a5550-a69b-4c1e-b538-8062096df0ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1212
        }
      },
      "source": [
        "s[output].bind(bf, tvm.thread_axis(\"blockIdx.z\"))\n",
        "s[output].bind(by, tvm.thread_axis(\"blockIdx.y\"))\n",
        "s[output].bind(bx, tvm.thread_axis(\"blockIdx.x\"))\n",
        "s[output].bind(vf, tvm.thread_axis(\"vthread\"))\n",
        "s[output].bind(vy, tvm.thread_axis(\"vthread\"))\n",
        "s[output].bind(vx, tvm.thread_axis(\"vthread\"))\n",
        "s[output].bind(tf, tvm.thread_axis(\"threadIdx.z\"))\n",
        "s[output].bind(ty, tvm.thread_axis(\"threadIdx.y\"))\n",
        "s[output].bind(tx, tvm.thread_axis(\"threadIdx.x\"))\n",
        "s[OL].compute_at(s[output], tx)\n",
        "print(tvm.lower(s, [input, kernel, output], simple_mode=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "// attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "allocate pad_temp.shared[float32 * 4608]\n",
            "// attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "allocate pad_temp.shared.local[float32 * 4608]\n",
            "// attr [kernel.shared.local] storage_scope = \"local\"\n",
            "allocate kernel.shared.local[float32 * 4608]\n",
            "produce pad_temp.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)] = tvm_if_then_else((((((1 - threadIdx.y) <= ax2) && (ax2 < (8 - threadIdx.y))) && ((1 - threadIdx.x) <= ax3)) && (ax3 < (8 - threadIdx.x))), data[(((((((ax1*7) + ax2) + threadIdx.y)*7) + ax3) + threadIdx.x) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce pad_temp.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        pad_temp.shared.local[((((ax1*3) + ax2)*3) + ax3)] = pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)] = kernel[((((((((blockIdx.z*8) + threadIdx.z)*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        kernel.shared.local[((((ax1*3) + ax2)*3) + ax3)] = pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  // attr [iter_var(blockIdx.z, , blockIdx.z)] thread_extent = 64\n",
            "  // attr [compute.local] storage_scope = \"local\"\n",
            "  allocate compute.local[float32 * 1]\n",
            "  // attr [iter_var(blockIdx.y, , blockIdx.y)] thread_extent = 1\n",
            "  // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 1\n",
            "  // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 8\n",
            "  // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "  // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 7\n",
            "  produce compute.local {\n",
            "    compute.local[0] = 0.000000f\n",
            "    for (rc, 0, 512) {\n",
            "      for (ry, 0, 3) {\n",
            "        for (rx, 0, 3) {\n",
            "          compute.local[0] = (compute.local[0] + (pad_temp.shared.local[((((rc*3) + ry)*3) + rx)]*kernel.shared.local[((((rc*3) + ry)*3) + rx)]))\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  compute[((((((blockIdx.z*8) + threadIdx.z)*7) + threadIdx.y)*7) + threadIdx.x)] = compute.local[0]\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSicMUUukiu2",
        "colab_type": "text"
      },
      "source": [
        "Next, we apply a tiling transformation over the reduction axes, using a series of loop axes splits followed by a reorder as in the previous case. With this arrangement of loop axes, we also define the points at which each cached tensor is prepared to be read or written with `compute_at`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT992Qu6ki6n",
        "colab_type": "code",
        "outputId": "8da6887d-2969-4b3c-9f16-10ed3b13848d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1082
        }
      },
      "source": [
        "# tile reduction axes\n",
        "n, f, y, x = s[OL].op.axis\n",
        "rc, ry, rx = s[OL].op.reduce_axis\n",
        "rc_factors = [512, 32, 1]\n",
        "rx_factors = [3, 3, 1]\n",
        "ry_factors = [3, 3, 1]\n",
        "rco, rcm = s[OL].split(rc, factor=rc_factors[1])\n",
        "rcm, rci = s[OL].split(rcm, factor=rc_factors[2])\n",
        "ryo, rym = s[OL].split(ry, factor=ry_factors[1])\n",
        "rym, ryi = s[OL].split(rym, factor=ry_factors[2])\n",
        "rxo, rxm = s[OL].split(rx, factor=rx_factors[1])\n",
        "rxm, rxi = s[OL].split(rxm, factor=rx_factors[2])\n",
        "s[OL].reorder(rco, ryo, rxo, rcm, rym, rxm, rci, ryi, rxi, n, f, y, x)\n",
        "\n",
        "s[AA].compute_at(s[OL], rxo)\n",
        "s[WW].compute_at(s[OL], rxo)\n",
        "s[AL].compute_at(s[OL], rxm)\n",
        "s[WL].compute_at(s[OL], rxm)\n",
        "\n",
        "print(tvm.lower(s, [input, kernel, output], simple_mode=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "produce compute {\n",
            "  // attr [iter_var(blockIdx.z, , blockIdx.z)] thread_extent = 64\n",
            "  // attr [compute.local] storage_scope = \"local\"\n",
            "  allocate compute.local[float32 * 1]\n",
            "  // attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "  allocate pad_temp.shared[float32 * 2592]\n",
            "  // attr [kernel.shared] storage_scope = \"shared\"\n",
            "  allocate kernel.shared[float32 * 2304]\n",
            "  // attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "  allocate pad_temp.shared.local[float32 * 1]\n",
            "  // attr [kernel.shared.local] storage_scope = \"local\"\n",
            "  allocate kernel.shared.local[float32 * 1]\n",
            "  // attr [iter_var(blockIdx.y, , blockIdx.y)] thread_extent = 1\n",
            "  // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 1\n",
            "  // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 8\n",
            "  // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "  // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 7\n",
            "  produce compute.local {\n",
            "    compute.local[0] = 0.000000f\n",
            "    for (rc.outer, 0, 16) {\n",
            "      produce pad_temp.shared {\n",
            "        for (ax1, 0, 32) {\n",
            "          for (ax2, 0, 9) {\n",
            "            for (ax3, 0, 9) {\n",
            "              pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)] = tvm_if_then_else(((((1 <= ax2) && (ax2 < 8)) && (1 <= ax3)) && (ax3 < 8)), data[(((((((rc.outer*32) + ax1)*7) + ax2)*7) + ax3) + -8)], 0.000000f)\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      produce kernel.shared {\n",
            "        for (ax0, 0, 8) {\n",
            "          for (ax1, 0, 32) {\n",
            "            for (ax2, 0, 3) {\n",
            "              for (ax3, 0, 3) {\n",
            "                kernel.shared[((((((ax0*32) + ax1)*3) + ax2)*3) + ax3)] = kernel[((((((((((blockIdx.z*8) + ax0)*16) + rc.outer)*32) + ax1)*3) + ax2)*3) + ax3)]\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      for (rc.inner.outer, 0, 32) {\n",
            "        for (ry.inner.outer, 0, 3) {\n",
            "          for (rx.inner.outer, 0, 3) {\n",
            "            produce pad_temp.shared.local {\n",
            "              pad_temp.shared.local[0] = pad_temp.shared[((((((rc.inner.outer*9) + threadIdx.y) + ry.inner.outer)*9) + threadIdx.x) + rx.inner.outer)]\n",
            "            }\n",
            "            produce kernel.shared.local {\n",
            "              kernel.shared.local[0] = kernel.shared[((((((threadIdx.z*32) + rc.inner.outer)*3) + ry.inner.outer)*3) + rx.inner.outer)]\n",
            "            }\n",
            "            compute.local[0] = (compute.local[0] + (pad_temp.shared.local[0]*kernel.shared.local[0]))\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  compute[((((((blockIdx.z*8) + threadIdx.z)*7) + threadIdx.y)*7) + threadIdx.x)] = compute.local[0]\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfBPrhkRmMKC",
        "colab_type": "text"
      },
      "source": [
        "Cooperative fetching:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdW3ZMB9mMVA",
        "colab_type": "code",
        "outputId": "efd15b7e-4bc1-4a97-8b75-acf10afb4d00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1433
        }
      },
      "source": [
        "# cooperative fetching\n",
        "for load in [AA, WW]:\n",
        "    n, f, y, x = s[load].op.axis \n",
        "    fused = s[load].fuse(n, f, y, x)\n",
        "    tz, fused = s[load].split(fused, nparts=tile_f_factors[2])\n",
        "    ty, fused = s[load].split(fused, nparts=tile_y_factors[2])\n",
        "    tx, fused = s[load].split(fused, nparts=tile_x_factors[2])\n",
        "    s[load].bind(tz, tvm.thread_axis(\"threadIdx.z\"))\n",
        "    s[load].bind(ty, tvm.thread_axis(\"threadIdx.y\"))\n",
        "    s[load].bind(tx, tvm.thread_axis(\"threadIdx.x\"))\n",
        "print(tvm.lower(s, [input, kernel, output], simple_mode=True))\n",
        "# tune unroll\n",
        "#s[output].pragma(kernel_scope, 'auto_unroll_max_step', cfg['auto_unroll_max_step'].val)\n",
        "#s[output].pragma(kernel_scope, 'unroll_explicit', cfg['unroll_explicit'].val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "produce compute {\n",
            "  // attr [iter_var(blockIdx.z, , blockIdx.z)] thread_extent = 64\n",
            "  // attr [compute.local] storage_scope = \"local\"\n",
            "  allocate compute.local[float32 * 1]\n",
            "  // attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "  allocate pad_temp.shared[float32 * 2592]\n",
            "  // attr [kernel.shared] storage_scope = \"shared\"\n",
            "  allocate kernel.shared[float32 * 2304]\n",
            "  // attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "  allocate pad_temp.shared.local[float32 * 1]\n",
            "  // attr [kernel.shared.local] storage_scope = \"local\"\n",
            "  allocate kernel.shared.local[float32 * 1]\n",
            "  // attr [iter_var(blockIdx.y, , blockIdx.y)] thread_extent = 1\n",
            "  // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 1\n",
            "  // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 8\n",
            "  // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "  // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 7\n",
            "  produce compute.local {\n",
            "    compute.local[0] = 0.000000f\n",
            "    for (rc.outer, 0, 16) {\n",
            "      produce pad_temp.shared {\n",
            "        // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 8\n",
            "        // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "        // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 7\n",
            "        for (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner, 0, 7) {\n",
            "          if (likely(((threadIdx.z*4) < (32 - ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)/81))))) {\n",
            "            if (likely(((threadIdx.z*36) < (288 - ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)/9))))) {\n",
            "              if (likely(((threadIdx.z*324) < (((2592 - ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) - (threadIdx.x*7)) - (threadIdx.y*47))))) {\n",
            "                if (likely(((threadIdx.y*47) < ((324 - ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) - (threadIdx.x*7))))) {\n",
            "                  if (likely(((threadIdx.x*7) < (47 - ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)))) {\n",
            "                    pad_temp.shared[((threadIdx.z*324) + (((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner))] = tvm_if_then_else(((((1 <= (((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)/9) % 9)) && ((((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)/9) % 9) < 8)) && (1 <= ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) % 9))) && (((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) % 9) < 8)), data[((((((((((((threadIdx.z*4) + ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)/81))/32)*16) + rc.outer)*32) + (((threadIdx.z*4) + ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)/81)) % 32))*7) + (((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) % 81)/9))*7) + ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) % 9)) + -8)], 0.000000f)\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      produce kernel.shared {\n",
            "        // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 8\n",
            "        // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "        // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 7\n",
            "        for (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner, 0, 6) {\n",
            "          if (likely(((((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3))/96) < (8 - threadIdx.z)))) {\n",
            "            if (likely(((threadIdx.z*32) < (256 - (((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3))/3))))) {\n",
            "              if (likely(((threadIdx.z*96) < (((768 - (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3)) - (threadIdx.x*2)) - (threadIdx.y*14))))) {\n",
            "                if (likely(((threadIdx.z*288) < (((2304 - ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) - (threadIdx.x*6)) - (threadIdx.y*42))))) {\n",
            "                  if (likely(((((threadIdx.y*7) + threadIdx.x)*6) < (288 - ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)))) {\n",
            "                    if (likely(((blockIdx.z*8) < ((512 - threadIdx.z) - (((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3))/96))))) {\n",
            "                      kernel.shared[((((threadIdx.z*96) + ((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3)))*3) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner % 3))] = kernel[(((((((((blockIdx.z*8) + (((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3))/96)) + threadIdx.z)*16) + rc.outer)*96) + (((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3)) % 96))*3) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner % 3))]\n",
            "                    }\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      for (rc.inner.outer, 0, 32) {\n",
            "        for (ry.inner.outer, 0, 3) {\n",
            "          for (rx.inner.outer, 0, 3) {\n",
            "            produce pad_temp.shared.local {\n",
            "              pad_temp.shared.local[0] = pad_temp.shared[((((((rc.inner.outer*9) + threadIdx.y) + ry.inner.outer)*9) + threadIdx.x) + rx.inner.outer)]\n",
            "            }\n",
            "            produce kernel.shared.local {\n",
            "              kernel.shared.local[0] = kernel.shared[((((((threadIdx.z*32) + rc.inner.outer)*3) + ry.inner.outer)*3) + rx.inner.outer)]\n",
            "            }\n",
            "            compute.local[0] = (compute.local[0] + (pad_temp.shared.local[0]*kernel.shared.local[0]))\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  compute[((((((blockIdx.z*8) + threadIdx.z)*7) + threadIdx.y)*7) + threadIdx.x)] = compute.local[0]\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRZZDR5wrRA_",
        "colab_type": "text"
      },
      "source": [
        "Compile and run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmYLCxlbrRKW",
        "colab_type": "code",
        "outputId": "f09a834e-42ce-43f7-95a5-de0e37c01bb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# check correctness\n",
        "a_np = np.random.uniform(size=(N, CI, H, W)).astype(np.float32)\n",
        "w_np = np.random.uniform(size=(CO, CI, KH, KW)).astype(np.float32)\n",
        "c_np = conv2d_nchw_python(a_np, w_np, stride, padding)\n",
        "\n",
        "with tvm.target.create('cuda'):\n",
        "    manual_conv2d = tvm.build(s, [input, kernel, output])\n",
        "    \n",
        "ctx = tvm.gpu()\n",
        "a_tvm = tvm.nd.array(a_np, ctx=ctx)\n",
        "w_tvm = tvm.nd.array(w_np, ctx=ctx)\n",
        "c_tvm = tvm.nd.empty(c_np.shape, ctx=ctx)\n",
        "manual_conv2d(a_tvm, w_tvm, c_tvm)\n",
        "\n",
        "tvm.testing.assert_allclose(c_np, c_tvm.asnumpy(), rtol=1e-2)\n",
        "\n",
        "evaluator = manual_conv2d.time_evaluator(manual_conv2d.entry_name, ctx, number=400)\n",
        "mean = evaluator(a_tvm, w_tvm, c_tvm).mean\n",
        "print(\"complexity: \", autotvm.task.task.compute_flop(s))\n",
        "print(\"Time cost of this operator: %f\" % mean)\n",
        "print(\"GFLOPS:\", (autotvm.task.task.compute_flop(s)/mean)/1e9)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "complexity:  231501312\n",
            "Time cost of this operator: 0.000970\n",
            "GFLOPS: 238.66136044884905\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R0J9j5m_CBY",
        "colab_type": "text"
      },
      "source": [
        "Using a schedule template instead of a manually defined schedule\n",
        "====================================================\n",
        "\n",
        "Next, we show that we can avoid the magic numbers used previously and instead leave them as free variables to be decided by a tuner. This change relieves the burden of tuning on the schedule writer and also potentially opens up a much large space for optimization.\n",
        "\n",
        "Note that we wrap the schedule in a function to leverage the `@autotvm.template` decorator for automated tuning.\n",
        "Tunable parameters that were previously manually specified are now passed using the `cfg` variable in the schedule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sgFW_oi_OHr",
        "colab_type": "code",
        "outputId": "9d0a7d27-22c6-43bd-e468-9657544b7330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "@autotvm.template\n",
        "def conv2d_no_batching(N, H, W, CO, CI, KH, KW, stride, padding):\n",
        "    assert N == 1, \"Only consider batch_size = 1 in this template\"\n",
        "\n",
        "    data = tvm.placeholder((N, CI, H, W), name='data')\n",
        "    kernel = tvm.placeholder((CO, CI, KH, KW), name='kernel')\n",
        "    conv = topi.nn.conv2d_nchw(data, kernel, stride, padding, dilation=1, out_dtype='float32')\n",
        "    s = tvm.create_schedule([conv.op])\n",
        "\n",
        "    ##### space definition begin #####\n",
        "    n, f, y, x = s[conv].op.axis\n",
        "    rc, ry, rx = s[conv].op.reduce_axis\n",
        "\n",
        "    cfg = autotvm.get_config()\n",
        "    cfg.define_split(\"tile_f\", f, num_outputs=4)\n",
        "    cfg.define_split(\"tile_y\", y, num_outputs=4)\n",
        "    cfg.define_split(\"tile_x\", x, num_outputs=4)\n",
        "    cfg.define_split(\"tile_rc\", rc, num_outputs=3)\n",
        "    cfg.define_split(\"tile_ry\", ry, num_outputs=3)\n",
        "    cfg.define_split(\"tile_rx\", rx, num_outputs=3)\n",
        "    cfg.define_knob(\"auto_unroll_max_step\", [0, 512, 1500])\n",
        "    cfg.define_knob(\"unroll_explicit\", [0, 1])\n",
        "    ##### space definition end #####\n",
        "\n",
        "    # inline padding\n",
        "    pad_data = s[conv].op.input_tensors[0]\n",
        "    s[pad_data].compute_inline()\n",
        "    data, raw_data = pad_data, data\n",
        "\n",
        "    output = conv\n",
        "    OL = s.cache_write(conv, 'local')\n",
        "\n",
        "    # create cache stage\n",
        "    AA = s.cache_read(data, 'shared', [OL])\n",
        "    WW = s.cache_read(kernel, 'shared', [OL])\n",
        "    AL = s.cache_read(AA, 'local', [OL])\n",
        "    WL = s.cache_read(WW, 'local', [OL])\n",
        "\n",
        "    # tile and bind spatial axes\n",
        "    n, f, y, x = s[output].op.axis\n",
        "    bf, vf, tf, fi = cfg[\"tile_f\"].apply(s, output, f)\n",
        "    by, vy, ty, yi = cfg[\"tile_y\"].apply(s, output, y)\n",
        "    bx, vx, tx, xi = cfg[\"tile_x\"].apply(s, output, x)\n",
        "    kernel_scope = n  # this is the scope to attach global config inside this kernel\n",
        "\n",
        "    s[output].bind(bf, tvm.thread_axis(\"blockIdx.z\"))\n",
        "    s[output].bind(by, tvm.thread_axis(\"blockIdx.y\"))\n",
        "    s[output].bind(bx, tvm.thread_axis(\"blockIdx.x\"))\n",
        "    s[output].bind(vf, tvm.thread_axis(\"vthread\"))\n",
        "    s[output].bind(vy, tvm.thread_axis(\"vthread\"))\n",
        "    s[output].bind(vx, tvm.thread_axis(\"vthread\"))\n",
        "    s[output].bind(tf, tvm.thread_axis(\"threadIdx.z\"))\n",
        "    s[output].bind(ty, tvm.thread_axis(\"threadIdx.y\"))\n",
        "    s[output].bind(tx, tvm.thread_axis(\"threadIdx.x\"))\n",
        "    s[output].reorder(n, bf, by, bx, vf, vy, vx, tf, ty, tx, fi, yi, xi)\n",
        "    s[OL].compute_at(s[output], tx)\n",
        "\n",
        "    # tile reduction axes\n",
        "    n, f, y, x = s[OL].op.axis\n",
        "    rc, ry, rx = s[OL].op.reduce_axis\n",
        "    rco, rcm, rci = cfg['tile_rc'].apply(s, OL, rc)\n",
        "    ryo, rym, ryi = cfg['tile_rx'].apply(s, OL, ry)\n",
        "    rxo, rxm, rxi = cfg['tile_ry'].apply(s, OL, rx)\n",
        "    s[OL].reorder(rco, ryo, rxo, rcm, rym, rxm, rci, ryi, rxi, n, f, y, x)\n",
        "\n",
        "    s[AA].compute_at(s[OL], rxo)\n",
        "    s[WW].compute_at(s[OL], rxo)\n",
        "    s[AL].compute_at(s[OL], rxm)\n",
        "    s[WL].compute_at(s[OL], rxm)\n",
        "\n",
        "    # cooperative fetching\n",
        "    for load in [AA, WW]:\n",
        "        n, f, y, x = s[load].op.axis\n",
        "        fused = s[load].fuse(n, f, y, x)\n",
        "        tz, fused = s[load].split(fused, nparts=cfg[\"tile_f\"].size[2])\n",
        "        ty, fused = s[load].split(fused, nparts=cfg[\"tile_y\"].size[2])\n",
        "        tx, fused = s[load].split(fused, nparts=cfg[\"tile_x\"].size[2])\n",
        "        s[load].bind(tz, tvm.thread_axis(\"threadIdx.z\"))\n",
        "        s[load].bind(ty, tvm.thread_axis(\"threadIdx.y\"))\n",
        "        s[load].bind(tx, tvm.thread_axis(\"threadIdx.x\"))\n",
        "\n",
        "    # tune unroll\n",
        "    s[output].pragma(kernel_scope, 'auto_unroll_max_step', cfg['auto_unroll_max_step'].val)\n",
        "    s[output].pragma(kernel_scope, 'unroll_explicit', cfg['unroll_explicit'].val)\n",
        "\n",
        "    return s, [raw_data, kernel, conv]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-d9fdd46594cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mautotvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconv2d_no_batching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Only consider batch_size = 1 in this template\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tvm/python/tvm/autotvm/task/task.py\u001b[0m in \u001b[0;36mtemplate\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconfig_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Do not support kwargs in template function call\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tvm/python/tvm/autotvm/task/task.py\u001b[0m in \u001b[0;36m_do_reg\u001b[0;34m(myf)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTASK_TABLE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverride\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             raise ValueError(\n\u001b[0;32m--> 142\u001b[0;31m                 \"Key %s is already registered\" % name)\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0mTASK_TABLE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmyf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Key conv2d_no_batching is already registered"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-CFfMD6Ie1O",
        "colab_type": "text"
      },
      "source": [
        "Start Infrastructure for Tuning (tracker)\n",
        "==============================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ch6kPVaIfDo",
        "colab_type": "code",
        "outputId": "5b5586c2-0f43-4b8a-abf3-403d55796116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%script bash --bg --out output --err error\n",
        "PYTHONPATH=/tvm/python:$PYTHONPATH && python3 -m tvm.exec.rpc_tracker --host 0.0.0.0 --port 9190 &\n",
        "while true; do\n",
        "  res=$(PYTHONPATH=/tvm/python:$PYTHONPATH && python3 -m tvm.exec.query_rpc_tracker --host 0.0.0.0 --port 9190 2>&1 | grep 'Cannot connect to tracker')\n",
        "  if [ \"$res\" == \"\" ]; then\n",
        "    echo \"OK @ \" $(date) \"...\" >> status.log\n",
        "  else\n",
        "    echo \"RESTARTING @ \" $(date) \"...\" >> status.log\n",
        "    PYTHONPATH=/tvm/python:$PYTHONPATH && python3 -m tvm.exec.rpc_tracker --host 0.0.0.0 --port 9190 &\n",
        "  fi\n",
        "  sleep 5\n",
        "done"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting job # 21 in a separate thread.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8v4NcPtIm8R",
        "colab_type": "text"
      },
      "source": [
        "Start Infrastructure for Tuning (server)\n",
        "==============================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liZMOINaIm_v",
        "colab_type": "code",
        "outputId": "acf90b34-d806-44a2-ebb9-ef08f750be6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%script bash --bg --out output2 --err error2\n",
        "while true; do\n",
        "echo \"started server at \" $(date) >> status.log\n",
        "PYTHONPATH=/tvm/python:/tvm/topi/python:$PYTHONPATH && python3 -m tvm.exec.rpc_server --key 1080ti --tracker 0.0.0.0:9190\n",
        "sleep 30\n",
        "done"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting job # 22 in a separate thread.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRzivXLpKtCk",
        "colab_type": "text"
      },
      "source": [
        "Check the status of the tracker\n",
        "========================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6R8SoScKtJ6",
        "colab_type": "code",
        "outputId": "9bfb71d4-bdda-4c30-c97d-adb6802b5223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        }
      },
      "source": [
        "! cat status.log | tail\n",
        "! PYTHONPATH=/tvm/python:$PYTHONPATH && python3 -m tvm.exec.query_rpc_tracker --host 0.0.0.0 --port 9190 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OK @  Wed Jun 5 01:43:55 UTC 2019 ...\n",
            "OK @  Wed Jun 5 01:44:01 UTC 2019 ...\n",
            "OK @  Wed Jun 5 01:44:06 UTC 2019 ...\n",
            "OK @  Wed Jun 5 01:44:12 UTC 2019 ...\n",
            "OK @  Wed Jun 5 01:44:17 UTC 2019 ...\n",
            "OK @  Wed Jun 5 01:44:22 UTC 2019 ...\n",
            "started server at  Wed Jun 5 01:44:54 UTC 2019\n",
            "started server at  Wed Jun 5 01:45:09 UTC 2019\n",
            "started server at  Wed Jun 5 01:45:32 UTC 2019\n",
            "RESTARTING @  Wed Jun 5 01:45:32 UTC 2019 ...\n",
            "Tracker address 0.0.0.0:9190\n",
            "\n",
            "Server List\n",
            "----------------------------\n",
            "server-address\tkey\n",
            "----------------------------\n",
            "127.0.0.1:58364\tserver:1080ti\n",
            "----------------------------\n",
            "\n",
            "Queue Status\n",
            "------------------------------\n",
            "key      total  free  pending\n",
            "------------------------------\n",
            "1080ti   1      1     0      \n",
            "------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfznWuPiIVQw",
        "colab_type": "text"
      },
      "source": [
        "Run Tuning\n",
        "========="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPgLaW8eIYdR",
        "colab_type": "code",
        "outputId": "38a72e20-290c-4fe1-fb7a-c8df473071ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39617
        }
      },
      "source": [
        "# logging config (for printing tuning log to screen)\n",
        "logging.getLogger('autotvm').setLevel(logging.DEBUG)\n",
        "logging.getLogger('autotvm').addHandler(logging.StreamHandler(sys.stdout))\n",
        "\n",
        "# the last layer in resnet\n",
        "N, H, W, CO, CI, KH, KW, strides, padding = 1, 7, 7, 512, 512, 3, 3, (1, 1), (1, 1)\n",
        "task = autotvm.task.create(conv2d_no_batching,\n",
        "                           args=(N, H, W, CO, CI, KH, KW, strides, padding),\n",
        "                           target='cuda')\n",
        "print(task.config_space)\n",
        "print(task.flop)\n",
        "\n",
        "# Use local gpu, measure 10 times for every config to reduce variance\n",
        "# The timeout of compiling a program is 10 seconds, the timeout for running is 4 seconds\n",
        "measure_option = autotvm.measure_option(\n",
        "    builder=autotvm.LocalBuilder(),\n",
        "            runner=autotvm.RPCRunner(\n",
        "            '1080ti',  # change the device key to your key\n",
        "            '0.0.0.0', 9190,\n",
        "            number=256, repeat=3, timeout=1, min_repeat_ms=50)\n",
        ")\n",
        "\n",
        "# Begin tuning, log records to file `conv2d.log`\n",
        "# During tuning we will also try many invalid configs, so you are expected to\n",
        "# see many error reports. As long as you can see non-zero GFLOPS, it is okay.\n",
        "tuner = autotvm.tuner.XGBTuner(task, feature_type='knob')\n",
        "tuner.tune(n_trial=512,\n",
        "           measure_option=measure_option,\n",
        "           callbacks=[autotvm.callback.log_to_file('conv2d.log')])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConfigSpace (len=10454400, space_map=\n",
            "   0 tile_f: Split(policy=all, product=512, num_outputs=4) len=220\n",
            "   1 tile_y: Split(policy=all, product=7, num_outputs=4) len=4\n",
            "   2 tile_x: Split(policy=all, product=7, num_outputs=4) len=4\n",
            "   3 tile_rc: Split(policy=all, product=512, num_outputs=3) len=55\n",
            "   4 tile_ry: Split(policy=all, product=3, num_outputs=3) len=3\n",
            "   5 tile_rx: Split(policy=all, product=3, num_outputs=3) len=3\n",
            "   6 auto_unroll_max_step: OtherOption([0, 512, 1500]) len=3\n",
            "   7 unroll_explicit: OtherOption([0, 1]) len=2\n",
            ")\n",
            "231501312\n",
            "Get devices for measurement successfully!\n",
            "Get devices for measurement successfully!\n",
            "Get devices for measurement successfully!\n",
            "Get devices for measurement successfully!\n",
            "Get devices for measurement successfully!\n",
            "Get devices for measurement successfully!\n",
            "Get devices for measurement successfully!\n",
            "Get devices for measurement successfully!\n"
            ],
          "name": "stdout"
        }
      ]
    }
  ]
}
